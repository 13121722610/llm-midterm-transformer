# compare_generation.py
import torch
import argparse
from data import load_data
from model import DecoderOnlyTransformer

def generate_text(model, tokenizer, prompt, max_new_tokens=50, temperature=0.8, top_k=20, top_p=0.9):
    """ç”Ÿæˆæ–‡æœ¬å‡½æ•°"""
    model.eval()
    device = next(model.parameters()).device
    
    idx = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long).to(device)
    
    for _ in range(max_new_tokens):
        idx_cond = idx if idx.size(1) <= 256 else idx[:, -256:]
        
        logits = model(idx_cond)
        logits = logits[:, -1, :] / temperature
        
        # Top-k é‡‡æ ·
        if top_k is not None and top_k > 0:
            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
            logits[logits < v[:, [-1]]] = -float('Inf')
        
        # Top-p (æ ¸é‡‡æ ·)
        if top_p is not None and top_p < 1.0:
            sorted_logits, sorted_indices = torch.sort(logits, descending=True)
            cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
            
            sorted_indices_to_remove = cumulative_probs > top_p
            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
            sorted_indices_to_remove[..., 0] = 0
            
            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
            logits[indices_to_remove] = -float('Inf')
        
        probs = torch.softmax(logits, dim=-1)
        
        if torch.all(probs == 0):
            probs = torch.ones_like(probs) / probs.size(-1)
            
        next_id = torch.multinomial(probs, num_samples=1)
        idx = torch.cat([idx, next_id], dim=1)
    
    return tokenizer.decode(idx[0].tolist())

def compare_models_generation():
    """å¯¹æ¯”ä¸åŒå±‚æ•°æ¨¡å‹çš„ç”Ÿæˆæ•ˆæœ"""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    tokenizer, _, _ = load_data()
    
    # ä¸åŒå±‚æ•°çš„æ¨¡å‹é…ç½®
    model_configs = [
        {'layers': 2, 'path': 'checkpoints/ablation_layers_2_best.pt', 'name': '2å±‚æ¨¡å‹'},
        {'layers': 4, 'path': 'checkpoints/ablation_layers_4_best.pt', 'name': '4å±‚æ¨¡å‹'},
        {'layers': 6, 'path': 'checkpoints/ablation_layers_6_best.pt', 'name': '6å±‚æ¨¡å‹(3è½®)'},
        {'layers': 6, 'path': 'checkpoints/decoder_only_improved_best.pt', 'name': '6å±‚æ¨¡å‹(5è½®)'}
    ]
    
    # æµ‹è¯•ä¸åŒçš„prompt
    test_prompts = [
        "To be, or not to be",
        "Once upon a time",
        "The future of AI",
        "Love is",
        "In the beginning"
    ]
    
    results = []
    
    for config in model_configs:
        print(f"\n{'='*60}")
        print(f"ğŸ§ª æµ‹è¯•: {config['name']}")
        print(f"{'='*60}")
        
        try:
            # åˆ›å»ºæ¨¡å‹
            model = DecoderOnlyTransformer(
                vocab_size=tokenizer.vocab_size,
                d_model=256,
                n_layer=config['layers'],
                n_head=8,
                d_ff=1024,
                max_seq_len=256,
                dropout=0.1
            ).to(device)
            
            # åŠ è½½æƒé‡
            model.load_state_dict(torch.load(config['path'], map_location=device))
            print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {config['path']}")
            
            # å¯¹æ¯ä¸ªpromptç”Ÿæˆæ–‡æœ¬
            for prompt in test_prompts:
                print(f"\nğŸ“ Prompt: \"{prompt}\"")
                generated = generate_text(model, tokenizer, prompt, max_new_tokens=30)
                print(f"ğŸ“¤ ç”Ÿæˆ: {generated}")
                
                results.append({
                    'model': config['name'],
                    'layers': config['layers'],
                    'prompt': prompt,
                    'generated_text': generated
                })
            
            # é‡Šæ”¾æ¨¡å‹å†…å­˜
            del model
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {config['path']} - {e}")
    
    # ä¿å­˜ç”Ÿæˆç»“æœ
    import pandas as pd
    results_df = pd.DataFrame(results)
    results_df.to_csv('results/tables/generation_comparison.csv', index=False)
    print(f"\nâœ… ç”Ÿæˆå¯¹æ¯”ç»“æœä¿å­˜è‡³: results/tables/generation_comparison.csv")
    
    return results

if __name__ == "__main__":
    compare_models_generation()
