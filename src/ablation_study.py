# src/ablation_study.py
import os
import time
import math
import json
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from torch.utils.data import DataLoader
from tqdm import tqdm
from data import load_data
from model import DecoderOnlyTransformer

class CharDataset:
    def __init__(self, data, block_size):
        self.data = data
        self.block_size = block_size

    def __len__(self):
        return len(self.data) - self.block_size

    def __getitem__(self, idx):
        x = self.data[idx:idx+self.block_size]
        y = self.data[idx+1:idx+self.block_size+1]
        return torch.from_numpy(x).long(), torch.from_numpy(y).long()

def ensure_directories():
    """ç¡®ä¿æ‰€æœ‰è¾“å‡ºç›®å½•å­˜åœ¨"""
    directories = [
        "checkpoints",
        "results/figures", 
        "results/tables",
        "results/logs"
    ]
    for directory in directories:
        os.makedirs(directory, exist_ok=True)

def load_pretrained_weights(model, pretrained_path="checkpoints/decoder_only_improved_best.pt"):
    """æ™ºèƒ½åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼Œå¤„ç†å±‚æ•°ä¸åŒ¹é…çš„æƒ…å†µ"""
    try:
        pretrained_state = torch.load(pretrained_path, map_location='cpu')
        model_state = model.state_dict()
        
        transferred_params = 0
        total_params = 0
        
        for name, param in pretrained_state.items():
            if name in model_state:
                if param.shape == model_state[name].shape:
                    model_state[name] = param
                    transferred_params += param.numel()
                else:
                    # å±‚æ•°ä¸åŒ¹é…ï¼Œå°è¯•æ™ºèƒ½åŒ¹é…
                    if 'layers' in name:
                        layer_num = int(name.split('.')[1])
                        if layer_num < len(model.layers):
                            model_state[name] = param
                            transferred_params += param.numel()
            total_params += param.numel()
        
        model.load_state_dict(model_state)
        transfer_rate = transferred_params / total_params * 100
        print(f"âœ… æˆåŠŸåŠ è½½é¢„è®­ç»ƒæƒé‡: {transfer_rate:.1f}% å‚æ•°")
        return True
    except Exception as e:
        print(f"âŒ åŠ è½½é¢„è®­ç»ƒæƒé‡å¤±è´¥: {e}")
        return False

def quick_evaluate_model(model, val_loader, criterion, device):
    """å¿«é€Ÿè¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_val_loss = 0.0
    with torch.no_grad():
        for xb, yb in val_loader:
            xb, yb = xb.to(device), yb.to(device)
            logits = model(xb)
            loss = criterion(logits.view(-1, logits.size(-1)), yb.view(-1))
            total_val_loss += loss.item()
    
    return total_val_loss / len(val_loader)

def train_ablation_model(model, tokenizer, config, model_name, device):
    """è®­ç»ƒå•ä¸ªæ¶ˆèå®éªŒæ¨¡å‹"""
    # å‡†å¤‡æ•°æ®åŠ è½½å™¨
    _, train_data, val_data = load_data()
    train_dataset = CharDataset(train_data, config['block_size'])
    val_dataset = CharDataset(val_data, config['block_size'])
    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'])
    
    # ä¼˜åŒ–å™¨é…ç½®
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=config['lr'],
        weight_decay=config['weight_decay']
    )
    
    criterion = nn.CrossEntropyLoss()
    
    # è®­ç»ƒè®°å½•
    best_val_loss = float('inf')
    train_losses = []
    val_losses = []
    
    print(f"å¼€å§‹è®­ç»ƒ {model_name}...")
    
    for epoch in range(config['epochs']):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_train_loss = 0.0
        
        for xb, yb in tqdm(train_loader, desc=f"Epoch {epoch+1}/{config['epochs']}"):
            xb, yb = xb.to(device), yb.to(device)
            
            logits = model(xb)
            loss = criterion(logits.view(-1, logits.size(-1)), yb.view(-1))
            
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            total_train_loss += loss.item()
        
        avg_train_loss = total_train_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # éªŒè¯é˜¶æ®µ
        avg_val_loss = quick_evaluate_model(model, val_loader, criterion, device)
        val_losses.append(avg_val_loss)
        
        print(f"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val PPL: {math.exp(avg_val_loss):.2f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), f"checkpoints/{model_name}_best.pt")
    
    return {
        'model_name': model_name,
        'layers': config['n_layer'],
        'best_val_loss': best_val_loss,
        'best_val_ppl': math.exp(best_val_loss),
        'final_val_loss': val_losses[-1],
        'final_val_ppl': math.exp(val_losses[-1]),
        'parameters': sum(p.numel() for p in model.parameters()),
        'train_epochs': config['epochs']
    }

def plot_ablation_results(results, original_result=None):
    """ç»˜åˆ¶æ¶ˆèå®éªŒç»“æœå›¾"""
    layers = [r['layers'] for r in results]
    val_ppls = [r['best_val_ppl'] for r in results]
    parameters = [r['parameters'] for r in results]
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    
    # å›°æƒ‘åº¦å¯¹æ¯”
    bars = ax1.bar([str(l) for l in layers], val_ppls, color=['#ff9999', '#66b3ff', '#99ff99'])
    ax1.set_xlabel('Transformerå±‚æ•°')
    ax1.set_ylabel('éªŒè¯å›°æƒ‘åº¦ (PPL)')
    ax1.set_title('ä¸åŒå±‚æ•°æ¶æ„çš„éªŒè¯å›°æƒ‘åº¦å¯¹æ¯”\n(æ¶ˆèå®éªŒ)')
    ax1.grid(True, alpha=0.3)
    
    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
    for bar, ppl in zip(bars, val_ppls):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                f'{ppl:.2f}', ha='center', va='bottom', fontweight='bold')
    
    # å¦‚æœæœ‰åŸå§‹6å±‚5è½®çš„ç»“æœï¼Œæ·»åŠ å‚è€ƒçº¿
    if original_result:
        ax1.axhline(y=original_result['best_val_ppl'], color='red', linestyle='--', 
                   label=f"åŸå§‹6å±‚5è½®: {original_result['best_val_ppl']:.2f}")
        ax1.legend()
    
    # å‚æ•°é‡å¯¹æ¯”
    ax2.bar([str(l) for l in layers], [p/1e6 for p in parameters], 
            color='orange', alpha=0.7)
    ax2.set_xlabel('Transformerå±‚æ•°')
    ax2.set_ylabel('å‚æ•°é‡ (ç™¾ä¸‡)')
    ax2.set_title('ä¸åŒå±‚æ•°æ¶æ„çš„å‚æ•°é‡å¯¹æ¯”')
    ax2.grid(True, alpha=0.3)
    
    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºå‚æ•°é‡
    for i, (layer, param) in enumerate(zip(layers, parameters)):
        ax2.text(i, param/1e6 + 0.1, f'{param/1e6:.1f}M', 
                ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('results/figures/layer_ablation_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("ğŸ“Š æ¶ˆèå®éªŒå¯¹æ¯”å›¾å·²ç”Ÿæˆ: results/figures/layer_ablation_comparison.png")

def load_original_results():
    """åŠ è½½åŸå§‹6å±‚5è½®æ¨¡å‹çš„è®­ç»ƒç»“æœ"""
    try:
        # ä»åŸå§‹ç»“æœæ–‡ä»¶åŠ è½½
        original_df = pd.read_csv("results/tables/decoder_only_improved_final_results.csv")
        original_result = {
            'model_name': 'decoder_only_improved_6L5E',
            'layers': 6,
            'best_val_loss': original_df['best_val_loss'].iloc[0],
            'best_val_ppl': original_df['best_val_ppl'].iloc[0],
            'final_val_loss': original_df['final_val_loss'].iloc[0],
            'final_val_ppl': original_df['final_val_ppl'].iloc[0],
            'parameters': original_df['parameters'].iloc[0],
            'train_epochs': original_df['total_epochs'].iloc[0],
            'note': 'åŸå§‹6å±‚5è½®å®Œæ•´è®­ç»ƒ'
        }
        print("âœ… æˆåŠŸåŠ è½½åŸå§‹6å±‚5è½®æ¨¡å‹ç»“æœ")
        return original_result
    except Exception as e:
        print(f"âŒ åŠ è½½åŸå§‹ç»“æœå¤±è´¥: {e}")
        return None

def layer_ablation_study():
    """å±‚æ•°æ¶ˆèå®éªŒä¸»å‡½æ•°"""
    ensure_directories()
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    print("ğŸš€ å¼€å§‹å±‚æ•°æ¶ˆèå®éªŒ...")
    print("ğŸ“ å®éªŒç›®æ ‡: æ¯”è¾ƒä¸åŒå±‚æ•°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“")
    print("â±ï¸  æ¯ä¸ªé…ç½®è®­ç»ƒ3ä¸ªepochè¿›è¡Œå¿«é€Ÿå¯¹æ¯”\n")
    
    # åŠ è½½åŸå§‹6å±‚5è½®æ¨¡å‹ç»“æœ
    original_result = load_original_results()
    if original_result:
        print(f"ğŸ“Š åŸå§‹6å±‚5è½®æ¨¡å‹: å›°æƒ‘åº¦ {original_result['best_val_ppl']:.2f}, è®­ç»ƒè½®æ¬¡ {original_result['train_epochs']}")
    
    # åŠ è½½æ•°æ®
    tokenizer, _, _ = load_data()
    
    # æµ‹è¯•ä¸åŒçš„å±‚æ•°é…ç½®
    layer_configs = [2, 4, 6]  # 2å±‚(æµ…) vs 4å±‚(ä¸­) vs 6å±‚(æ·±)
    results = []
    
    for n_layer in layer_configs:
        print(f"\n{'='*60}")
        print(f"ğŸ§ª æ¶ˆèå®éªŒ: {n_layer} å±‚ Decoder-Only Transformer")
        print(f"{'='*60}")
        
        # åˆ›å»ºæ¨¡å‹é…ç½®
        config = {
            'd_model': 256,
            'n_layer': n_layer,
            'n_head': 8,
            'd_ff': 1024,
            'block_size': 256,
            'batch_size': 32,
            'epochs': 3,  # å¿«é€Ÿè®­ç»ƒ3ä¸ªepoch
            'lr': 1e-4,
            'dropout': 0.1,
            'weight_decay': 0.01
        }
        
        # åˆ›å»ºæ¨¡å‹
        model = DecoderOnlyTransformer(
            tokenizer.vocab_size,
            d_model=config['d_model'],
            n_layer=config['n_layer'],
            n_head=config['n_head'],
            d_ff=config['d_ff'],
            max_seq_len=config['block_size'],
            dropout=config['dropout']
        ).to(device)
        
        # å°è¯•åŠ è½½é¢„è®­ç»ƒæƒé‡
        load_success = load_pretrained_weights(model)
        
        # è®­ç»ƒæ¨¡å‹
        model_name = f"ablation_layers_{n_layer}"
        result = train_ablation_model(model, tokenizer, config, model_name, device)
        results.append(result)
        
        print(f"âœ… {n_layer}å±‚æ¨¡å‹å®Œæˆ - æœ€ä½³éªŒè¯å›°æƒ‘åº¦: {result['best_val_ppl']:.2f}")
    
    # ä¿å­˜æ¶ˆèå®éªŒç»“æœ
    ablation_df = pd.DataFrame(results)
    ablation_file = "results/tables/layer_ablation_results.csv"
    ablation_df.to_csv(ablation_file, index=False)
    print(f"\nğŸ¯ å±‚æ•°æ¶ˆèå®éªŒå®Œæˆ! ç»“æœä¿å­˜è‡³: {ablation_file}")
    
    # ç”Ÿæˆæ¶ˆèå®éªŒå¯¹æ¯”å›¾
    plot_ablation_results(results, original_result)
    
    # æ‰“å°æ€»ç»“
    print(f"\nğŸ“‹ æ¶ˆèå®éªŒæ€»ç»“:")
    print(f"{'å±‚æ•°':<8} {'å›°æƒ‘åº¦':<10} {'å‚æ•°é‡':<12} {'è®­ç»ƒè½®æ¬¡'}")
    print(f"{'-'*40}")
    for result in results:
        print(f"{result['layers']:<8} {result['best_val_ppl']:<10.2f} {result['parameters']/1e6:<12.1f}M {result['train_epochs']:<10}")
    
    if original_result:
        print(f"\nğŸ“Š åŸå§‹6å±‚5è½®æ¨¡å‹å‚è€ƒ:")
        print(f"å±‚æ•°: 6, å›°æƒ‘åº¦: {original_result['best_val_ppl']:.2f}, å‚æ•°é‡: {original_result['parameters']/1e6:.1f}M, è®­ç»ƒè½®æ¬¡: {original_result['train_epochs']}")
    
    return results

if __name__ == "__main__":
    layer_ablation_study()
