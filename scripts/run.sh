#!/bin/bash

# =============================================================================
# LLM Midterm Transformer - å®Œæ•´å®éªŒæµç¨‹
# ä½œè€…ï¼šå­£æœˆå«
# å­¦å·ï¼š25120410
# æè¿°ï¼šä¸€é”®å¤ç°è®­ç»ƒã€æ¶ˆèå®éªŒå’Œç”Ÿæˆå¯¹æ¯”
# =============================================================================

set -e  # é‡åˆ°ä»»ä½•é”™è¯¯ç«‹å³é€€å‡º

echo "================================================================"
echo "           LLM Midterm Transformer (å®Œæ•´å®éªŒæµç¨‹)"
echo "================================================================"
echo "å¼€å§‹æ—¶é—´: $(date)"
echo ""

# ============================ å‚æ•°é…ç½® ============================
SEED=42
PROJECT_DIR="/data0/yhji/llm-midterm-transformer"
PYTHON_PATH="$PROJECT_DIR/src"
GPU_ID="3"  # æŒ‡å®šä½¿ç”¨GPU 3

# ============================ ç¯å¢ƒæ£€æŸ¥ ============================
echo "=== æ­¥éª¤1: ç¯å¢ƒæ£€æŸ¥ ==="

# æ£€æŸ¥condaç¯å¢ƒ
if ! conda info --envs | grep -q "llm_toy"; then
    echo "åˆ›å»ºcondaç¯å¢ƒ: llm_toy"
    conda create -n llm_toy python=3.10 -y
else
    echo "æ‰¾åˆ°condaç¯å¢ƒ: llm_toy"
fi

# æ¿€æ´»ç¯å¢ƒ
echo "æ¿€æ´»condaç¯å¢ƒ..."
eval "$(conda shell.bash hook)"
conda activate llm_toy

# æ£€æŸ¥Python
echo "Pythonç‰ˆæœ¬: $(python --version)"

# ============================ ä¾èµ–å®‰è£… ============================
echo ""
echo "=== æ­¥éª¤2: å®‰è£…ä¾èµ– ==="

# æ£€æŸ¥æ˜¯å¦æœ‰requirements.txt
if [ -f "$PROJECT_DIR/requirements.txt" ]; then
    echo "ä½¿ç”¨requirements.txtå®‰è£…ä¾èµ–..."
    pip install -r "$PROJECT_DIR/requirements.txt"
else
    echo "æœªæ‰¾åˆ°requirements.txtï¼Œå®‰è£…æ ¸å¿ƒä¾èµ–..."
    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
    pip install numpy tqdm matplotlib requests tokenizers pandas
fi

# ============================ ç¯å¢ƒè®¾ç½® ============================
echo ""
echo "=== æ­¥éª¤3: ç¯å¢ƒè®¾ç½® ==="

# è®¾ç½®Pythonè·¯å¾„
export PYTHONPATH="$PROJECT_DIR"
export PYTHONHASHSEED=$SEED

# æ£€æŸ¥å¹¶æ˜¾ç¤ºç°æœ‰ç›®å½•
echo "é¡¹ç›®ç›®å½•ç»“æ„:"
for dir in "checkpoints" "results" "data"; do
    if [ -d "$PROJECT_DIR/$dir" ]; then
        echo "âœ… $PROJECT_DIR/$dir/ (å·²å­˜åœ¨)"
    else
        echo "ğŸ“ åˆ›å»ºç›®å½•: $PROJECT_DIR/$dir/"
        mkdir -p "$PROJECT_DIR/$dir"
    fi
done

echo "é¡¹ç›®ç›®å½•: $PROJECT_DIR"
echo "Pythonè·¯å¾„: $PYTHONPATH"
echo "éšæœºç§å­: $SEED"
echo "ä½¿ç”¨GPU: $GPU_ID"

# ============================ æ•°æ®å‡†å¤‡ ============================
echo ""
echo "=== æ­¥éª¤4: æ•°æ®å‡†å¤‡ ==="

cd "$PYTHON_PATH"

echo "ä¸‹è½½è®­ç»ƒæ•°æ®..."
python -c "from data import load_data; tokenizer, train_data, val_data = load_data(); print(f'æ•°æ®åŠ è½½å®Œæˆ!\\nè¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}\\nè®­ç»ƒæ•°æ®é•¿åº¦: {len(train_data)}\\néªŒè¯æ•°æ®é•¿åº¦: {len(val_data)}')"

# ============================ æ¨¡å‹è®­ç»ƒ ============================
echo ""
echo "=== æ­¥éª¤5: æ¨¡å‹è®­ç»ƒ ==="

# æ£€æŸ¥æ˜¯å¦å·²æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
if [ -f "$PROJECT_DIR/src/checkpoints/decoder_only_best.pt" ] || [ -f "$PROJECT_DIR/src/checkpoints/decoder_only_improved_best.pt" ]; then
    echo "âœ… å‘ç°å·²è®­ç»ƒçš„Decoder-Onlyæ¨¡å‹"
    echo "è·³è¿‡è®­ç»ƒæ­¥éª¤..."
else
    echo "å¼€å§‹è®­ç»ƒDecoder-Only Transformeræ¨¡å‹..."
    echo "è¿™å°†éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…..."
    CUDA_VISIBLE_DEVICES=$GPU_ID python train.py
fi

# ============================ æ¶ˆèå®éªŒæ£€æŸ¥ ============================
echo ""
echo "=== æ­¥éª¤6: æ¶ˆèå®éªŒæ£€æŸ¥ ==="

# æ£€æŸ¥æ¶ˆèå®éªŒæ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´
ablation_models_exist=true
for layers in 2 4 6; do
    if [ ! -f "$PROJECT_DIR/src/checkpoints/ablation_layers_${layers}_best.pt" ]; then
        ablation_models_exist=false
        echo "âŒ ç¼ºå°‘æ¶ˆèå®éªŒæ¨¡å‹: ablation_layers_${layers}_best.pt"
        break
    fi
done

# æ£€æŸ¥æ¶ˆèå®éªŒç»“æœæ–‡ä»¶
if [ -f "$PROJECT_DIR/src/results/tables/layer_ablation_results.csv" ] && $ablation_models_exist; then
    echo "âœ… æ¶ˆèå®éªŒå·²å®Œæˆï¼Œè·³è¿‡æ­¤æ­¥éª¤"
    echo "ğŸ“Š æ¶ˆèå®éªŒç»“æœ:"
    python -c "
import pandas as pd
try:
    df = pd.read_csv('results/tables/layer_ablation_results.csv')
    print(f'å±‚æ•°å¯¹æ¯”: 2å±‚({df[df[\"layers\"]==2][\"best_val_ppl\"].iloc[0]:.2f}) | 4å±‚({df[df[\"layers\"]==4][\"best_val_ppl\"].iloc[0]:.2f}) | 6å±‚({df[df[\"layers\"]==6][\"best_val_ppl\"].iloc[0]:.2f})')
except:
    print('æ— æ³•è¯»å–æ¶ˆèå®éªŒç»“æœ')
"
else
    echo "å¼€å§‹å±‚æ•°æ¶ˆèå®éªŒ..."
    echo "è¿™å°†è®­ç»ƒ2å±‚ã€4å±‚ã€6å±‚æ¨¡å‹è¿›è¡Œå¯¹æ¯”..."
    CUDA_VISIBLE_DEVICES=$GPU_ID python ablation_study.py
fi

# ============================ å›¾è¡¨ç”Ÿæˆ ============================
echo ""
echo "=== æ­¥éª¤7: å›¾è¡¨ç”Ÿæˆ ==="

# æ£€æŸ¥æ˜¯å¦å·²æœ‰è‹±æ–‡å›¾è¡¨
if [ -f "$PROJECT_DIR/src/results/figures/layer_ablation_comparison_english.png" ]; then
    echo "âœ… è‹±æ–‡å›¾è¡¨å·²å­˜åœ¨ï¼Œè·³è¿‡ç”Ÿæˆ"
else
    echo "ç”Ÿæˆæ¶ˆèå®éªŒè‹±æ–‡å›¾è¡¨..."
    python -c "
import pandas as pd
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
try:
    results_df = pd.read_csv('results/tables/layer_ablation_results.csv')
    results = results_df.to_dict('records')
    layers = [r['layers'] for r in results]
    val_ppls = [r['best_val_ppl'] for r in results]
    parameters = [r['parameters'] for r in results]
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    bars = ax1.bar([str(l) for l in layers], val_ppls, color=['#ff9999', '#66b3ff', '#99ff99'])
    ax1.set_xlabel('Number of Layers')
    ax1.set_ylabel('Validation Perplexity (PPL)')
    ax1.set_title('Layer Ablation: Perplexity Comparison')
    ax1.grid(True, alpha=0.3)
    for bar, ppl in zip(bars, val_ppls):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1, f'{ppl:.2f}', ha='center', va='bottom', fontweight='bold')
    ax2.bar([str(l) for l in layers], [p/1e6 for p in parameters], color='orange', alpha=0.7)
    ax2.set_xlabel('Number of Layers')
    ax2.set_ylabel('Parameters (Millions)')
    ax2.set_title('Layer Ablation: Parameter Count')
    ax2.grid(True, alpha=0.3)
    for i, (layer, param) in enumerate(zip(layers, parameters)):
        ax2.text(i, param/1e6 + 0.1, f'{param/1e6:.1f}M', ha='center', va='bottom', fontweight='bold')
    plt.tight_layout()
    plt.savefig('results/figures/layer_ablation_comparison_english.png', dpi=300, bbox_inches='tight')
    plt.close()
    print('âœ… è‹±æ–‡å›¾è¡¨å·²ç”Ÿæˆ')
except Exception as e:
    print(f'âŒ å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}')
"
fi

# ============================ ç”Ÿæˆå¯¹æ¯” ============================
echo ""
echo "=== æ­¥éª¤8: ç”Ÿæˆå¯¹æ¯”æµ‹è¯• ==="

# æ£€æŸ¥æ˜¯å¦å·²æœ‰ç”Ÿæˆå¯¹æ¯”ç»“æœï¼Œä½†å¼ºåˆ¶é‡æ–°è¿è¡Œ
if [ -f "$PROJECT_DIR/src/results/tables/generation_comparison.csv" ] && [ "$1" != "--force" ]; then
    echo "âœ… ç”Ÿæˆå¯¹æ¯”å·²å®Œæˆï¼Œè·³è¿‡æ­¤æ­¥éª¤"
    echo "ğŸ“ ç”Ÿæˆå¯¹æ¯”æ ·æœ¬:"
    python -c "
import pandas as pd
try:
    df = pd.read_csv('results/tables/generation_comparison.csv')
    # æ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹çš„æ ·æœ¬
    for model in df['model'].unique():
        sample = df[df['model']==model].head(1)
        if not sample.empty:
            prompt = sample['prompt'].iloc[0]
            text = sample['generated_text'].iloc[0]
            print(f'{model}: {text[:50]}...')
except:
    print('æ— æ³•è¯»å–ç”Ÿæˆå¯¹æ¯”ç»“æœ')
"
else
    echo "å¼€å§‹æ¨¡å‹ç”Ÿæˆå¯¹æ¯”æµ‹è¯•..."
    
    # åˆ›å»ºç”Ÿæˆå¯¹æ¯”è„šæœ¬
    cat > compare_generation.py << 'EOF'
import torch
import pandas as pd
from data import load_data
from model import DecoderOnlyTransformer

def generate_text(model, tokenizer, prompt, max_new_tokens=50, temperature=0.8, top_k=20, top_p=0.9):
    model.eval()
    device = next(model.parameters()).device
    idx = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long).to(device)
    
    for _ in range(max_new_tokens):
        idx_cond = idx if idx.size(1) <= 256 else idx[:, -256:]
        logits = model(idx_cond)
        logits = logits[:, -1, :] / temperature
        
        if top_k is not None and top_k > 0:
            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
            logits[logits < v[:, [-1]]] = -float('Inf')
        
        if top_p is not None and top_p < 1.0:
            sorted_logits, sorted_indices = torch.sort(logits, descending=True)
            cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
            sorted_indices_to_remove = cumulative_probs > top_p
            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
            sorted_indices_to_remove[..., 0] = 0
            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
            logits[indices_to_remove] = -float('Inf')
        
        probs = torch.softmax(logits, dim=-1)
        if torch.all(probs == 0):
            probs = torch.ones_like(probs) / probs.size(-1)
        next_id = torch.multinomial(probs, num_samples=1)
        idx = torch.cat([idx, next_id], dim=1)
    
    return tokenizer.decode(idx[0].tolist())

def compare_models_generation():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    tokenizer, _, _ = load_data()
    
    model_configs = [
        {'layers': 2, 'path': 'checkpoints/ablation_layers_2_best.pt', 'name': '2å±‚æ¨¡å‹'},
        {'layers': 4, 'path': 'checkpoints/ablation_layers_4_best.pt', 'name': '4å±‚æ¨¡å‹'},
        {'layers': 6, 'path': 'checkpoints/ablation_layers_6_best.pt', 'name': '6å±‚æ¨¡å‹(3è½®)'},
        {'layers': 6, 'path': 'checkpoints/decoder_only_improved_best.pt', 'name': '6å±‚æ¨¡å‹(5è½®)'}
    ]
    
    test_prompts = [
        "To be, or not to be",
        "Once upon a time", 
        "The future of AI",
        "Love is",
        "In the beginning"
    ]
    
    results = []
    
    print("ğŸ¯ æ¶ˆèå®éªŒæ¨¡å‹ç”Ÿæˆå¯¹æ¯”åˆ†æ")
    print("=" * 70)
    
    for config in model_configs:
        print(f"\nğŸ§ª {config['name']} ({config['layers']}å±‚)")
        print("-" * 50)
        
        try:
            model = DecoderOnlyTransformer(
                vocab_size=tokenizer.vocab_size,
                d_model=256,
                n_layer=config['layers'],
                n_head=8,
                d_ff=1024,
                max_seq_len=256,
                dropout=0.1
            ).to(device)
            
            model.load_state_dict(torch.load(config['path'], map_location=device, weights_only=False))
            
            for prompt in test_prompts:
                generated = generate_text(model, tokenizer, prompt, max_new_tokens=30)
                print(f"ğŸ“ '{prompt}'")
                print(f"   â†’ {generated}")
                
                results.append({
                    'model': config['name'],
                    'layers': config['layers'], 
                    'prompt': prompt,
                    'generated_text': generated
                })
            
            del model
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
    
    results_df = pd.DataFrame(results)
    results_df.to_csv('results/tables/generation_comparison.csv', index=False)
    print(f"\nâœ… ç”Ÿæˆå¯¹æ¯”å®Œæˆ! ç»“æœä¿å­˜è‡³: results/tables/generation_comparison.csv")

if __name__ == "__main__":
    compare_models_generation()
EOF

    CUDA_VISIBLE_DEVICES=$GPU_ID python compare_generation.py
    rm compare_generation.py  # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
fi

# ============================ åŸºç¡€ç”Ÿæˆæµ‹è¯• ============================
echo ""
echo "=== æ­¥éª¤9: åŸºç¡€ç”Ÿæˆæµ‹è¯• ==="

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
if [ -f "$PROJECT_DIR/src/checkpoints/decoder_only_best.pt" ] || [ -f "$PROJECT_DIR/src/checkpoints/decoder_only_improved_best.pt" ]; then
    echo "åŸºç¡€ç”Ÿæˆæµ‹è¯•..."
    
    prompts=("To be, or not to be" "Once upon a time" "The future of AI" "Love is" "In the beginning")
    
    for prompt in "${prompts[@]}"; do
        echo ""
        echo "Prompt: \"$prompt\""
        echo "ç”Ÿæˆç»“æœ:"
        CUDA_VISIBLE_DEVICES=$GPU_ID python generate.py --prompt "$prompt" --temperature 0.8 --max_new_tokens 50
        echo "----------------------------------------"
    done
else
    echo "âŒ é”™è¯¯: æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶"
    exit 1
fi

# ============================ ç»“æœæ±‡æ€» ============================
echo ""
echo "=== æ­¥éª¤10: å®éªŒç»“æœæ±‡æ€» ==="
echo "å®Œæˆæ—¶é—´: $(date)"
echo ""
echo "ğŸ‰ å®éªŒå®Œæˆæ±‡æ€»:"
echo "âœ… ç¯å¢ƒé…ç½®å®Œæˆ"
echo "âœ… ä¾èµ–å®‰è£…å®Œæˆ" 
echo "âœ… æ•°æ®å‡†å¤‡å®Œæˆ"
echo "âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ"
echo "âœ… æ¶ˆèå®éªŒå®Œæˆ"
echo "âœ… å›¾è¡¨ç”Ÿæˆå®Œæˆ"
echo "âœ… ç”Ÿæˆå¯¹æ¯”å®Œæˆ"
echo "âœ… åŸºç¡€ç”Ÿæˆæµ‹è¯•å®Œæˆ"
echo ""
echo "ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:"
echo "æ¨¡å‹æ–‡ä»¶:"
ls -la "$PROJECT_DIR/src/checkpoints/" | grep -E "(decoder_only|ablation_layers)" | head -10
echo ""
echo "ç»“æœæ–‡ä»¶:"
ls -la "$PROJECT_DIR/src/results/tables/"
ls -la "$PROJECT_DIR/src/results/figures/" | grep -E "(ablation|comparison)" | head -10
echo ""
echo "================================================================"
echo "                    å®Œæ•´å®éªŒæµç¨‹æ‰§è¡Œå®Œæ¯•ï¼"
echo "================================================================"
